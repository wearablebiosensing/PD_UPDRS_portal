{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5e98ad3",
   "metadata": {},
   "source": [
    "### Jupyter Notebook for MDPI itex glove data analysis\n",
    "# Steps: \n",
    "1. Check if Python libraries listed in imports (I used Python 3.8)\n",
    "2. Local copy of I-Tex glove dataset (Set variable dset_path in code cell below with dataset folder path in your PC)\n",
    "3. Run cells sequentially to extract features from LG, RG \n",
    "4. Evaluate feature significance with SHAP \n",
    "\n",
    "## Todos:\n",
    "1. Extract features suggested with MDPI reviewers [V,S] (Decrease in amplitude within activity, speed)\n",
    "2. Train using cross validation? (Or do proper train-test file split) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e9ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import signal\n",
    "from scipy.fft import rfft, rfftfreq\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import datetime\n",
    "import scipy\n",
    "import plotly.express as px\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from itertools import cycle\n",
    "import plotly.express as px\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import time\n",
    "\n",
    "#Hyper params\n",
    "# Base path of dataset \n",
    "dset_path = \"/Users/shehjarsadhu/Desktop/UniversityOfRhodeIsland/Graduate/WBL/Project_IOTEX/iotex-glove/PD\"\n",
    "\n",
    "np.random.seed(123)\n",
    "prom_val= (0.01, 4)\n",
    "width_val = (2,20)\n",
    "Dist_val = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdff1ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsfel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_peak_frequency(data, sampling_rate,n=1024):\n",
    "    \"\"\"Compute the peak frequency.\n",
    "\n",
    "    Requires Scipy\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 1d-array\n",
    "      Input signal in the time-domain.\n",
    "    sampling_rate : float\n",
    "      Sampling frequency of the data.\n",
    "    n : float\n",
    "      FFT points \n",
    "      (1024,2048)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    abs(peak_freq * sampling_rate) : float\n",
    "      Peak frequency.\n",
    "    \"\"\"\n",
    "    data = signal.detrend(data)\n",
    "    fft_data = np.fft.fft(data,n)\n",
    "#     freqs = np.fft.fftfreq(len(data))\n",
    "    freqs = np.fft.fftfreq(n)\n",
    "\n",
    "    peak_coefficient = np.argmax(np.abs(fft_data))\n",
    "    peak_freq = freqs[peak_coefficient]\n",
    "    return abs(peak_freq * sampling_rate)\n",
    "    \n",
    "def NormalizeData(data):\n",
    "    '''\n",
    "    Normalize data with max and min\n",
    "    '''\n",
    "\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def sampen(L, m, r):\n",
    "    '''\n",
    "    Sample entropy for given array L\n",
    "    '''\n",
    "    N = len(L)\n",
    "    B = 0.0\n",
    "    A = 0.0\n",
    "    \n",
    "    \n",
    "    # Split time series and save all templates of length m\n",
    "    xmi = np.array([L[i : i + m] for i in range(N - m)])\n",
    "    xmj = np.array([L[i : i + m] for i in range(N - m + 1)])\n",
    "\n",
    "    # Save all matches minus the self-match, compute B\n",
    "    B = np.sum([np.sum(np.abs(xmii - xmj).max(axis=1) <= r) - 1 for xmii in xmi])\n",
    "\n",
    "    # Similar for computing A\n",
    "    m += 1\n",
    "    xm = np.array([L[i : i + m] for i in range(N - m + 1)])\n",
    "\n",
    "    A = np.sum([np.sum(np.abs(xmi - xm).max(axis=1) <= r) - 1 for xmi in xm])\n",
    "\n",
    "    # Return SampEn\n",
    "    return -np.log(A / B)\n",
    "\n",
    "\n",
    "def peak_valley_analysis(data, Prominence=prom_val, Width = width_val,Distance=Dist_val):  \n",
    "    '''\n",
    "    Peak valley analysis for given data array, prominence value, width, minimum distance\n",
    "    Uses signal.find_peaks    \n",
    "    '''\n",
    "    data = signal.detrend(data)\n",
    "    peaks, properties = signal.find_peaks(data, prominence=Prominence, width=Width,distance=Distance)\n",
    "    valley, properties = signal.find_peaks(data*-1, prominence=Prominence, width=Width,distance=Distance)        \n",
    "            \n",
    "    p2p_dist = np.diff(peaks)\n",
    "    mean_p2p_dist = np.mean(p2p_dist)\n",
    "    std_p2p_dist = np.std(p2p_dist)\n",
    "    num_pk = len(peaks)\n",
    "    num_val = len(valley)\n",
    "\n",
    "#     if(num_pk == num_val):\n",
    "#         p2v_dist = np.abs(peaks-valley)\n",
    "#         mean_p2v_dist = np.mean(p2v_dist)\n",
    "#         std_p2v_dist = np.std(p2v_dist)\n",
    "#         p2v_height = np.abs(data[peaks]-data[valley])\n",
    "#         mean_p2v_height = np.mean(p2v_height)\n",
    "#         std_p2v_height = np.std(p2v_height)\n",
    "#     elif(num_pk > num_val):\n",
    "#         peaks = peaks[peaks<np.max(valley)]\n",
    "#         peaks = peaks[:num_val]\n",
    "#         p2v_dist = np.abs(peaks-valley)\n",
    "#         mean_p2v_dist = np.mean(p2v_dist)\n",
    "#         std_p2v_dist = np.std(p2v_dist)\n",
    "#         p2v_height = np.abs(data[valley]-data[peaks])\n",
    "#         mean_p2v_height = np.mean(p2v_height)\n",
    "#         std_p2v_height = np.std(p2v_height)\n",
    "#     elif(num_pk<num_val):\n",
    "#         valley = valley[valley<np.max(peaks)]\n",
    "#         valley = valley[:num_pk]\n",
    "#         p2v_dist = np.abs(peaks-valley)\n",
    "#         mean_p2v_dist = np.mean(p2v_dist)\n",
    "#         std_p2v_dist = np.std(p2v_dist)\n",
    "#         p2v_height = np.abs(data[peaks]-data[valley])\n",
    "#         mean_p2v_height = np.mean(p2v_height)\n",
    "#         std_p2v_height = np.std(p2v_height)\n",
    "#     num_pk = len(peaks)\n",
    "    return [mean_p2p_dist,std_p2p_dist]\n",
    "\n",
    "def bandpower(data, band, fs=64,  method='welch', window_sec=None, relative=False):\n",
    "    \"\"\"Compute the average power of the signal x in a specific frequency band.\n",
    "\n",
    "    Requires MNE-Python >= 0.14.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 1d-array\n",
    "      Input signal in the time-domain.\n",
    "    fs : float\n",
    "      Sampling frequency of the data.\n",
    "    band : list\n",
    "      Lower and upper frequencies of the band of interest.\n",
    "    method : string\n",
    "      Periodogram method: 'welch' or 'multitaper'\n",
    "    window_sec : float\n",
    "      Length of each window in seconds. Useful only if method == 'welch'.\n",
    "      If None, window_sec = (1 / min(band)) * 2.\n",
    "    relative : boolean\n",
    "      If True, return the relative power (= divided by the total power of the signal).\n",
    "      If False (default), return the absolute power.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    bp : float\n",
    "      Absoluteor relative band power.\n",
    "    \"\"\"\n",
    "    \n",
    "#     from scipy.signal import welch\n",
    "#     from scipy.integrate import simps\n",
    "#     from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "    band = np.asarray(band)\n",
    "    low, high = band\n",
    "\n",
    "    # Compute the modified periodogram (Welch)\n",
    "    if method == 'welch':\n",
    "        if window_sec is not None:\n",
    "            nperseg = window_sec * fs\n",
    "        else:\n",
    "            nperseg = (2 / low) * fs\n",
    "\n",
    "        freqs, psd = signal.welch(data, fs, nperseg=nperseg)\n",
    "\n",
    "    # Frequency resolution\n",
    "    freq_res = freqs[1] - freqs[0]\n",
    "\n",
    "    # Find index of band in frequency vector\n",
    "    idx_band = np.logical_and(freqs >= low, freqs <= high)\n",
    "\n",
    "    # Integral approximation of the spectrum using parabola (Simpson's rule)\n",
    "    bp = scipy.integrate.simps(psd[idx_band], dx=freq_res)\n",
    "\n",
    "    if relative:\n",
    "        bp /= scipy.integrate.simps(psd, dx=freq_res)\n",
    "    return bp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "        '''\n",
    "        Butterworth filter for given low and high frequency, sampling rate (fs) and order\n",
    "        '''\n",
    "        nyq = 0.5 * fs\n",
    "        low = lowcut / nyq\n",
    "        high = highcut / nyq\n",
    "        sos = signal.butter(order, [low, high], analog=False, btype='band', output='sos')\n",
    "        return sos\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "\n",
    "        sos = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "        y = signal.sosfilt(sos, data)\n",
    "        return y\n",
    "    \n",
    "#     if any(res.strftime('%Y%m%d-%H%M%S') in word for word in text_files):\n",
    "#         print(res.strftime('%Y%m%d-%H%M%S') +' is there inside the list!')\n",
    "#         return str(word)\n",
    "#     else:\n",
    "#         print(res.strftime('%Y%m%d-%H%M%S') +' is not there inside the list')\n",
    "#         return 0\n",
    "#     filter_object = filter(lambda a: res.strftime('%Y%m%d-%H%M%S') in a, text_files)\n",
    "#     text_file  = [k for k in text_files if res.strftime('%Y%m%d-%H%M%S')+\"_patient_form.txt\" in k]\n",
    "        \n",
    "\n",
    "def stride_trick(a, stride_length, stride_step):\n",
    "    \"\"\"\n",
    "    apply framing using the stride trick from numpy.\n",
    "\n",
    "    Args:\n",
    "        a (array) : signal array.\n",
    "        stride_length (int) : length of the stride.\n",
    "        stride_step (int) : stride step.\n",
    "\n",
    "    Returns:\n",
    "        blocked/framed array.\n",
    "    \"\"\"\n",
    "    nrows = ((a.size - stride_length) // stride_step) + 1\n",
    "    n = a.strides[0]\n",
    "    return np.lib.stride_tricks.as_strided(a,\n",
    "                                           shape=(nrows, stride_length),\n",
    "                                           strides=(stride_step*n, n))\n",
    "\n",
    "\n",
    "def framing(sig, fs=64, win_len=0.25, win_hop=0.125):\n",
    "    \"\"\"\n",
    "    transform a signal into a series of overlapping frames (=Frame blocking).\n",
    "\n",
    "    Args:\n",
    "        sig     (array) : a normalized and detrended signal.\n",
    "        fs        (int) : the sampling frequency of the signal we are working with.\n",
    "                          Default is 64.\n",
    "        win_len (float) : window length in sec.\n",
    "                          Default is 0.025.\n",
    "        win_hop (float) : step between successive windows in sec.\n",
    "                          Default is 0.01.\n",
    "\n",
    "    Returns:\n",
    "        array of frames.\n",
    "        frame length.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "        Uses the stride trick to accelerate the processing.\n",
    "    \"\"\"\n",
    "    # run checks and assertions\n",
    "    if win_len < win_hop: print(\"ParameterError: win_len must be larger than win_hop.\")\n",
    "\n",
    "    # compute frame length and frame step (convert from seconds to samples)\n",
    "    frame_length = win_len * fs\n",
    "    frame_step = win_hop * fs\n",
    "    signal_length = len(sig)\n",
    "    frames_overlap = frame_length - frame_step\n",
    "\n",
    "    # compute number of frames and left sample in order to pad if needed to make\n",
    "    # sure all frames have equal number of samples  without truncating any samples\n",
    "    # from the original signal\n",
    "    rest_samples = np.abs(signal_length - frames_overlap) % np.abs(frame_length - frames_overlap)\n",
    "    pad_signal = np.append(sig, np.array([0] * int(frame_step - rest_samples) * int(rest_samples != 0.)))\n",
    "\n",
    "    # apply stride trick\n",
    "    frames = stride_trick(pad_signal, int(frame_length), int(frame_step))\n",
    "    return frames, frame_length\n",
    "\n",
    "\n",
    "def _calculate_normalized_short_time_energy(frames):\n",
    "    '''Calculates the short time FFT band energy'''\n",
    "\n",
    "    return np.sum(np.abs(np.fft.rfft(a=frames, n=len(frames)))**2, axis=-1) / len(frames)**2\n",
    "\n",
    "\n",
    "def naive_frame_energy_vad(sig, fs=64, threshold=0, win_len=0.25, win_hop=0.25, E0=1e7):\n",
    "    '''\n",
    "    For detecting signal activity (windowing activity data)\n",
    "    '''\n",
    "    # framing\n",
    "    frames, frames_len = framing(sig=sig, fs=fs, win_len=win_len, win_hop=win_hop)\n",
    "\n",
    "    # compute short time energies to get voiced frames\n",
    "    energy = _calculate_normalized_short_time_energy(frames)\n",
    "    log_energy = 10 * np.log10(energy / E0)\n",
    "\n",
    "    # normalize energy to 0 dB then filter and format\n",
    "    energy = scipy.signal.medfilt(log_energy, 5)\n",
    "    energy = np.repeat(energy, frames_len)\n",
    "#     print(energy.shape)\n",
    "    mean_energy = np.mean(energy)\n",
    "    threshold = np.average([mean_energy,np.amin(energy)])\n",
    "    # compute vad and get speech frames\n",
    "    vad     = np.array(energy > threshold, dtype=sig.dtype)\n",
    "    #find and remove re-detection\n",
    "    diff_vad = np.diff(vad)\n",
    "#     print(threshold,np.where(diff_vad==-1)[0].shape)\n",
    "    if(len(np.where(diff_vad==-1)[0])!=0):\n",
    "#         print(\"here\")\n",
    "        pos_neg = np.where(diff_vad==-1)[0]\n",
    "#         print(pos_neg)\n",
    "        vad[int(pos_neg[0]):] = 0\n",
    "#     print(threshold)\n",
    "\n",
    "\n",
    "    vframes = np.array(frames.flatten()[np.where(vad==1)], dtype=sig.dtype)\n",
    "    \n",
    "\n",
    "    return energy, vad, np.array(vframes, dtype=np.float64)\n",
    "\n",
    "\n",
    "def multi_plots(data, titles, fs, plot_rows, step=1, colors=[\"b\", \"r\", \"m\", \"g\", \"b\", \"y\"]):\n",
    "    # first fig\n",
    "    plt.figure()\n",
    "    plt.subplots(plot_rows, 1, figsize=(20, 10))\n",
    "    plt.subplots_adjust(left=0.125, right=0.9, bottom=0.1, top=0.99, wspace=0.4, hspace=0.99)\n",
    "\n",
    "    for i in range(plot_rows):\n",
    "        plt.subplot(plot_rows, 1, i+1)\n",
    "        y = data[i]\n",
    "        plt.plot([i/fs for i in range(0, len(y), step)], y, colors[i])\n",
    "        plt.gca().set_title(titles[i])\n",
    "    plt.show()\n",
    "\n",
    "    # second fig\n",
    "    sig, vad = data[0], data[-2]\n",
    "    # plot VAD and orginal signal\n",
    "    plt.subplots(1, 1, figsize=(20, 10))\n",
    "    plt.plot([i/fs for i in range(len(sig))], sig, label=\"Signal\")\n",
    "    plt.plot([i/fs for i in range(len(vad))], max(sig)*vad, label=\"VAD\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def zcr(sig):\n",
    "    ''' \n",
    "    Calculates zero crossing rate of a given signal after detrending and normalizing\n",
    "    '''\n",
    "    sig = signal.detrend(sig)\n",
    "    sig_norm = [sig- min(sig)]\n",
    "    mval = max(abs(i) for i in sig)\n",
    "    sig = np.array([i / mval for i in sig])\n",
    "    val = ((sig[:-1] * sig[1:]) < 0).sum()\n",
    "    return val\n",
    "\n",
    "def mean_freq(sig, fs):\n",
    "    '''\n",
    "    Mean frequency within given signal\n",
    "    '''    \n",
    "    Y = np.abs(np.fft.rfft(sig)) \n",
    "    power_spectrum = Y**2\n",
    "    freq = np.fft.rfftfreq(len(sig), fs)\n",
    "    # Then calculate (weighted) mean\n",
    "    return np.sum(freq * power_spectrum / np.sum(power_spectrum)) # This should equal to the formula in your image.\n",
    "\n",
    "def rmsValue(sig):\n",
    "    '''\n",
    "    Finds root mean square of a given signal\n",
    "    '''\n",
    "\n",
    "    square = 0\n",
    "    mean = 0.0\n",
    "    root = 0.0\n",
    "    n = len(sig)\n",
    "     \n",
    "    #Calculate square\n",
    "    for i in range(0,n):\n",
    "        square += (sig[i]**2)\n",
    "     \n",
    "    #Calculate Mean\n",
    "    mean = (square / (float)(len(sig)))\n",
    "     \n",
    "    #Calculate Root\n",
    "    root = math.sqrt(mean)\n",
    "     \n",
    "    return root\n",
    " \n",
    "# Provide flex sigs for analysis, \n",
    "# returns [ft_mean_p2p_dist,ft_std_p2p_dist,ft_mean_p2v_dist,ft_std_p2v_dist,ft_mean_p2v_height,ft_std_p2v_height,ft_pdenergy,ft_dysenergy,ft_mean_freq, ft_rms\n",
    "def kinetic_analysis(sig,fs,order,lf,hf,prom_val,width_val,Dist_val,detect_theshold=-20):\n",
    "    '''\n",
    "    Computes peak-valley locations and band energy within given flexion/inertial signal  (For hand flip, finger tap, finger to nose, hand open-close)\n",
    "\n",
    "    TODO: Find features for start middle and ending windows\n",
    "\n",
    "    '''\n",
    "    sig = signal.detrend(sig)\n",
    "    b, a = scipy.signal.butter(order, [lf*2/fs, hf*2/fs], 'band') \n",
    "    sig = scipy.signal.filtfilt(b, a, sig)\n",
    "    sig_feats = []\n",
    "    pdenergy = bandpower(sig, [5, 10], fs, 'welch',relative=True)\n",
    "    dysenergy = bandpower(sig, [2,5], fs, 'welch',relative=True)\n",
    "    energy, vad, detected = naive_frame_energy_vad(sig, fs=fs, threshold=detect_theshold,\n",
    "                                                     win_len=0.25, win_hop=0.25)            \n",
    "    sig_feats = peak_valley_analysis(detected, Prominence=prom_val, Width = width_val,Distance=Dist_val)\n",
    "    sig_feats.append(pdenergy)\n",
    "    sig_feats.append(dysenergy)\n",
    "    sig_psd = bandpower(sig, [2, 20], fs, 'welch',relative=True) \n",
    "    sig_feats.append(sig_psd)\n",
    "    sig_feats.append(mean_freq(sig, fs))\n",
    "    sig_feats.append(rmsValue(detected))\n",
    "                \n",
    "    return sig_feats\n",
    "    \n",
    "                \n",
    "    \n",
    "                \n",
    "# Provide inertial signals (acc[x,y,z] & gyr[x,y,z]) for either RH or HH task\n",
    "# Returns seg1_acc_pdenergy,seg1_acc_dysenergy,seg1_acc_df,seg1_acc_mf,seg1_acc_std,seg1_acc_sent,seg1_acc_psd,seg1_acc_rms\\,seg1_gyr_pdenergy,seg1_gyr_dys_energy,seg1_gyr_df,seg1_gyr_mf,seg1_gyr_std,seg1_gyr_sent,seg1_gyr_psd,seg1_gyr_rmsseg2_acc_pdenergy,seg2_acc_dysenergy,seg2_acc_df,seg2_acc_mf,seg2_acc_std,seg2_acc_sent,seg2_acc_psd,seg2_acc_rms\\,seg2_gyr_pdenergy,seg2_gyr_dysenergy,seg2_gyr_df,seg2_gyr_mf,seg2_gyr_std,seg2_gyr_sent,seg2_gyr_psd,seg2_gyr_rms\n",
    "def stationary_analysis(inertial_sigs,fs):\n",
    "    '''\n",
    "    Computes band energy and other signal features inertial signals  (For resting hands, hold out hands tasks)\n",
    "\n",
    "    TODO: Find features for start middle and ending windows\n",
    "\n",
    "    '''\n",
    "    inertial_sigs = np.transpose(inertial_sigs)\n",
    "#     print(inertial_sigs.shape)\n",
    "    sig_seg_len = len(inertial_sigs[0])//2\n",
    "    [x ** 2 for x in range(10) if x % 2 == 0]\n",
    "    acc_rms_raw = np.array([rmsValue(inertial_sigs[:3,i]) for i in range(inertial_sigs.shape[1])])\n",
    "\n",
    "#        np.sqrt(sum(inertial_sigs[0]**2))+ np.sqrt(sum(inertial_sigs[1]**2))+np.sqrt(sum(inertial_sigs[2]**2))\n",
    "    gyr_rms_raw = np.array([rmsValue(inertial_sigs[3:,i]) for i in range(inertial_sigs.shape[1])])\n",
    "\n",
    "    b, a = scipy.signal.butter(2, [1*2/fs, 30*2/fs], 'band') \n",
    "    # Based on : An automated methodology for levodopa-induced dyskinesia: Assessment based on gyroscope and accelerometer signals\n",
    "    acc_rms = scipy.signal.filtfilt(b, a, acc_rms_raw)\n",
    "    gyr_rms = scipy.signal.filtfilt(b, a, gyr_rms_raw)\n",
    "    r_val = r_val = np.mean(np.std(acc_rms))\n",
    "    sig_feats = []    \n",
    "    seg1_acc_pdenergy = bandpower(acc_rms[:sig_seg_len], [5, 10], fs, 'welch',relative=True)\n",
    "    sig_feats.append(seg1_acc_pdenergy)\n",
    "    seg1_acc_dysenergy = bandpower(acc_rms[:sig_seg_len], [2, 5], fs, 'welch',relative=True)\n",
    "    sig_feats.append(seg1_acc_dysenergy)\n",
    "    seg1_acc_df = extract_peak_frequency(acc_rms[:sig_seg_len],64)\n",
    "    sig_feats.append(seg1_acc_df)\n",
    "    seg1_acc_mf = mean_freq(acc_rms[:sig_seg_len], fs)\n",
    "    sig_feats.append(seg1_acc_mf)\n",
    "    seg1_acc_std = np.std(acc_rms[:sig_seg_len])\n",
    "    sig_feats.append(seg1_acc_std)\n",
    "    seg1_acc_sent = sampen(acc_rms[:sig_seg_len], m=2, r=r_val)\n",
    "    sig_feats.append(seg1_acc_sent)\n",
    "    seg1_acc_psd = bandpower(acc_rms_raw[:sig_seg_len], [2, 20], fs, 'welch',relative=True) \n",
    "    sig_feats.append(seg1_acc_psd)\n",
    "    \n",
    "    seg1_gyr_pdenergy = bandpower(gyr_rms[:sig_seg_len], [2, 5], fs, 'welch',relative=True)\n",
    "    sig_feats.append(seg1_gyr_pdenergy)\n",
    "    seg1_gyr_dys_energy = bandpower(gyr_rms[:sig_seg_len], [5, 10], fs, 'welch',relative=True)\n",
    "    sig_feats.append(seg1_gyr_dys_energy)\n",
    "    seg1_gyr_df = extract_peak_frequency(gyr_rms[:sig_seg_len],fs)\n",
    "    sig_feats.append(seg1_gyr_df)\n",
    "    seg1_gyr_mf = mean_freq(gyr_rms[:sig_seg_len], fs)\n",
    "    sig_feats.append(seg1_gyr_mf)\n",
    "    seg1_gyr_std = np.std(gyr_rms[:sig_seg_len])\n",
    "    sig_feats.append(seg1_gyr_std)\n",
    "    seg1_gyr_sent = sampen(gyr_rms[:sig_seg_len], m=2, r=r_val)\n",
    "    sig_feats.append(seg1_gyr_sent)\n",
    "    seg1_gyr_psd = bandpower(gyr_rms_raw[:sig_seg_len], [2, 20], fs, 'welch',relative=True) \n",
    "    sig_feats.append(seg1_gyr_psd)\n",
    "\n",
    "    seg2_acc_pdenergy = bandpower(acc_rms[sig_seg_len:], [2, 5], fs, 'welch',relative=True)\n",
    "    sig_feats.append(seg2_acc_pdenergy)\n",
    "    seg2_acc_dysenergy = bandpower(acc_rms[sig_seg_len:], [5, 10], fs, 'welch',relative=True)\n",
    "    sig_feats.append(seg2_acc_dysenergy)\n",
    "    seg2_acc_df = extract_peak_frequency(acc_rms[sig_seg_len:],fs)\n",
    "    sig_feats.append(seg2_acc_df)\n",
    "    seg2_acc_mf = mean_freq(acc_rms[sig_seg_len:], fs)\n",
    "    sig_feats.append(seg2_acc_mf)\n",
    "    seg2_acc_std = np.std(acc_rms[sig_seg_len:])\n",
    "    sig_feats.append(seg2_acc_std)\n",
    "    seg2_acc_sent = sampen(acc_rms[sig_seg_len:], m=2, r=r_val)\n",
    "    sig_feats.append(seg2_acc_sent)\n",
    "    seg2_acc_psd = bandpower(acc_rms_raw[sig_seg_len:], [2, 20], 64, 'welch',relative=True) \n",
    "    sig_feats.append(seg2_acc_psd)\n",
    "    sig_acc_zc = zcr(acc_rms)\n",
    "    sig_feats.append(sig_acc_zc)\n",
    "\n",
    "    seg2_gyr_pdenergy = bandpower(gyr_rms[sig_seg_len:], [2, 5], 64, 'welch',relative=True)\n",
    "    sig_feats.append(seg2_gyr_pdenergy)\n",
    "    seg2_gyr_dysenergy = bandpower(gyr_rms[sig_seg_len:], [5, 10], 64, 'welch',relative=True)\n",
    "    sig_feats.append(seg2_gyr_dysenergy)\n",
    "    seg2_gyr_df = extract_peak_frequency(gyr_rms[sig_seg_len:],64)\n",
    "    sig_feats.append(seg2_gyr_df)\n",
    "    seg2_gyr_mf = mean_freq(gyr_rms[sig_seg_len:], 64)\n",
    "    sig_feats.append(seg2_gyr_mf)\n",
    "    seg2_gyr_std = np.std(gyr_rms[sig_seg_len:])\n",
    "    sig_feats.append(seg2_gyr_std)\n",
    "    seg2_gyr_sent = sampen(gyr_rms[sig_seg_len:], m=2, r=r_val)\n",
    "    sig_feats.append(seg2_gyr_sent)\n",
    "    seg2_gyr_psd = bandpower(gyr_rms_raw[sig_seg_len:], [2, 20], 64, 'welch',relative=True) \n",
    "    sig_feats.append(seg2_gyr_psd)\n",
    "    sig_gyr_zc = zcr(gyr_rms)\n",
    "\n",
    "    sig_feats.append(sig_gyr_zc)\n",
    "\n",
    "    \n",
    "\n",
    "    return sig_feats\n",
    "\n",
    "                \n",
    "    \n",
    "\n",
    "\n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91031038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part No: 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dset_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-99152a260a7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Part No: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# csv_files = glob.glob('C:/Users/dan95/Desktop/Participant1/**/*.csv',recursive=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mdset_csv_fpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset_path\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'/Participant'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/**/*.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mlabel_text_fpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset_path\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'/Participant'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/**/*.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dset_path' is not defined"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "# print(\"Current working directory: {0}\".format(cwd))\n",
    "# Medication intake labels\n",
    "med_usage = {\n",
    "  \"0_to_1_hrs\": 0,\n",
    "  \"1_to_2_hrs\": 1,\n",
    "  \"2_to_3_hrs\": 2,\n",
    "  \"4_plus\":3,\n",
    "  \"1_to_2hrs\": 1,\n",
    "}\n",
    "channels = ['index']\n",
    "cnames = [\"ft_mean_p2p_dist\",\"ft_std_p2p_dist\",\n",
    "#\"ft_std_p2v_dist\",\"ft_mean_p2v_height\",\"ft_std_p2v_height\",\n",
    "\"ft_pdenergy\",\"ft_dysenergy\",\"ft_psd\",\"ft_mean_freq\",\"ft_rms\",\n",
    "\"hh_seg1_acc_pdenergy\",\"hh_seg1_acc_dysenergy\",\"hh_seg1_acc_df\",\n",
    "\"hh_seg1_acc_mf\",\"hh_seg1_acc_std\",\"hh_seg1_acc_sent\",\"hh_seg1_acc_psd\",\n",
    "\"hh_seg1_gyr_pdenergy\",\"hh_seg1_gyr_dysenergy\",\n",
    "\"hh_seg1_gyr_df\",\"hh_seg1_gyr_mf\",\"hh_seg1_gyr_std\",\"hh_seg1_gyr_sent\",\n",
    "\"hh_seg1_gyr_psd\",\"hh_seg2_acc_pdenergy\",\n",
    "\"hh_seg2_acc_dysenergy\",\"hh_seg2_acc_df\",\"hh_seg2_acc_mf\",\n",
    "\"hh_seg2_acc_std\",\"hh_seg2_acc_sent\",\"hh_seg2_acc_psd\", \"hh_acc_zc\",\n",
    "\"hh_seg2_gyr_pdenergy\",\"hh_seg2_gyr_dysenergy\",\"hh_seg2_gyr_df\",\"hh_seg2_gyr_mf\",\"hh_seg2_gyr_std\",\n",
    "\"hh_seg2_gyr_sent\",\"hh_seg2_gyr_psd\",\"hh_gyr_zc\",\n",
    "\"rh_seg1_acc_pdenergy\",\"rh_seg1_acc_dysenergy\",\"rh_seg1_acc_df\",\n",
    "\"rh_seg1_acc_mf\",\"rh_seg1_acc_std\",\"rh_seg1_acc_sent\",\"rh_seg1_acc_psd\",\n",
    "\"rh_seg1_gyr_pdenergy\",\"rh_seg1_gyr_dysenergy\",\n",
    "\"rh_seg1_gyr_df\",\"rh_seg1_gyr_mf\",\"rh_seg1_gyr_std\",\"rh_seg1_gyr_sent\",\n",
    "\"rh_seg1_gyr_psd\",\"rh_seg2_acc_pdenergy\",\n",
    "\"rh_seg2_acc_dysenergy\",\"rh_seg2_acc_df\",\"rh_seg2_acc_mf\",\n",
    "\"rh_seg2_acc_std\",\"rh_seg2_acc_sent\",\"rh_seg2_acc_psd\",\"rh_acc_zc\",\n",
    "\"rh_seg2_gyr_pdenergy\",\"rh_seg2_gyr_dysenergy\",\"rh_seg2_gyr_df\",\"rh_seg2_gyr_mf\",\"rh_seg2_gyr_std\",\n",
    "\"rh_seg2_gyr_sent\",\"rh_seg2_gyr_psd\",\"rh_gyr_zc\", \n",
    "\"fn_mean_p2p_dist\",\"fn_std_p2p_dist\",#\"fn_mean_p2v_dist\",\n",
    "# \"fn_std_p2v_dist\",\"fn_mean_p2v_height\",\"fn_std_p2v_height\",\n",
    "\"fn_pdenergy\",\"fn_dysenergy\",\"fn_psd\",\"fn_mean_freq\",\"fn_rms\",\n",
    "\"hf_mean_p2p_dist\",\"hf_std_p2p_dist\",#\"hf_mean_p2v_dist\",\n",
    "#\"hf_std_p2v_dist\",\"hf_mean_p2v_height\",\"hf_std_p2v_height\",\n",
    "\"hf_pdenergy\",\"hf_dysenergy\",\"hf_psd\",\"hf_mean_freq\",\"hf_rms\",\"fno\",\"pno\",\"med_status\"]\n",
    "feat_rg = []\n",
    "feat_rg_tsfresh = []\n",
    "feat_rg_tsfresh_col = []\n",
    "for i in range(4):\n",
    "    pno = i+1\n",
    "    print(\"Part No: {}\".format(pno))\n",
    "    # csv_files = glob.glob('C:/Users/dan95/Desktop/Participant1/**/*.csv',recursive=True)\n",
    "    dset_csv_fpath = dset_path +'/Participant'+str(pno)+'/**/*.csv'\n",
    "    label_text_fpath = dset_path +'/Participant'+str(pno)+'/**/*.txt'\n",
    "\n",
    "    csv_files = glob.glob(dset_csv_fpath,recursive=True)\n",
    "    text_files = glob.glob(label_text_fpath,recursive=True)\n",
    "    rg_files = [s for s in csv_files if \"rg_\" in s]\n",
    "    lg_files = [s for s in csv_files if \"lg_\" in s]\n",
    "    lg_acts = [0,2,4,6,7,9]\n",
    "    rg_acts = [1,3,5,6,8,9]\n",
    "    print(\"RG cnt: {}, LG cnt: {}, Txt cnt: {}\".format(len(rg_files),len(lg_files),len(text_files)))# print(sorted(csv_files))\n",
    "    # print(glob.glob(csv_files))\n",
    "    # plot_channels = ['index','thumb','pitch','roll']\n",
    "    rg_act_no = 1\n",
    "    dist_ft = 0\n",
    "    tot_files = len(rg_files)\n",
    "    print(\"Total RG files: {}\".format(tot_files))\n",
    "    lf = 1\n",
    "    hf = 10\n",
    "    b, a = scipy.signal.butter(2, [lf*2/64, hf*2/64], 'band')\n",
    "    # plot results\n",
    "    # multi_plots(data=[sig, energy, vad, detected],\n",
    "    #             titles=[\"Input signal (activity + idle)\", \"Short time energy\",\n",
    "    #                     \"Activity detection\", \"Detected signal\"],\n",
    "    #             fs=64, plot_rows=4, step=1)\n",
    "\n",
    "\n",
    "    # Extract RG FT, HH, RH task features (Mean Peak Distance, Std of distance, Mean Peak Height, Std of height, PSD band energy[2-10Hz])\n",
    "    # Activity codes:\n",
    "    # 0: Left hand finger tap\n",
    "    # 1: Right hand finger tap\n",
    "    # 2: Left hand open close\n",
    "    # 3: Right hand open close\n",
    "    # 4: Left hand flip\n",
    "    # 5: Right hand flip \t\n",
    "    # 6: Both hands out\n",
    "    # 7: Left finger to nose\n",
    "    # 8: Right finger to nose\t\n",
    "    # 9: Hold out hand\n",
    "\n",
    "#     df_rg = pd.DataFrame([],columns =cnames)\n",
    "    for file_num in range(0,tot_files):\n",
    "        ch_rg = rg_files[file_num] #random.choice(rg_files)\n",
    "        df_rg = pd.read_csv(ch_rg,)\n",
    "        ch_form = text_files[file_num]     #Open questionnaire and find medication intake period \n",
    "        my_file = open(ch_form, \"r\")\n",
    "        medication_status = med_usage[my_file.read().split(\"\\n\")[4].split(\"=\")[1]]\n",
    "        print(\"File No: {}, Med: {}\" .format(file_num,medication_status))\n",
    "\n",
    "    #     sig = scipy.signal.filtfilt(b, a, df_rg[df_rg['activity'] == rg_act_no][channels].values)\n",
    "#         try:\n",
    "        index_sig = df_rg[df_rg['activity'] == rg_act_no]['index'].values\n",
    "        if len(index_sig)< 256:\n",
    "            print(\"File too small, skipping\")\n",
    "            continue\n",
    "        ## FT section\n",
    "        sig_feats = kinetic_analysis(index_sig,64,order=2,lf=1,hf=30, prom_val= (0.01, None), width_val = (2,20) ,Dist_val=5,detect_theshold=0)\n",
    "#         tsfresh_feats = extract_features(df_rg[[\"index\",\"pitch\",\"roll\"]],\n",
    "#                      default_fc_parameters=settings,\n",
    "#                      # we impute = remove all NaN features automatically\n",
    "#                      impute_function=impute)\n",
    "#         feat_rg_tsfresh = tsfresh_feats_ft.columns\n",
    "        print(\"FT feat len - {} \".format(len(sig_feats)))\n",
    "        ch_ = [\"ax\",\"ay\",\"az\",\"gx\",\"gy\",\"gz\"]\n",
    "        # sig = butter_bandpass_filter(df_rg[df_rg['activity'] == rg_act_no]['index'].values,0.1,30,64)\n",
    "        ## Extact features for HH task in sections 1 & 2 for accel and gyro [rms accel, dominant freq, mean freq,  ]\n",
    "        hh_feats = stationary_analysis(df_rg[df_rg['activity'] == 6][ch_].values,64)\n",
    "#             print(\"HH feat len - {} \".format(len(hh_feats)))\n",
    "        sig_feats.extend(hh_feats)\n",
    "#             print(\"tot feat len - {} \".format(len(sig_feats)))\n",
    "\n",
    "         ## Extact features for RH task in sections 1 & 2 for accel and gyro [rms accel, dominant freq, mean freq,  ]\n",
    "        rh_feats = stationary_analysis(df_rg[df_rg['activity'] == 9][ch_].values,64)\n",
    "#             print(\"RH feat len - {} \".format(len(rh_feats)))\n",
    "        sig_feats.extend(rh_feats)\n",
    "        fn_feats = kinetic_analysis(df_rg[df_rg['activity'] == 8]['pitch'],64,order=2,lf=1,hf=20, prom_val= (0.05,None), width_val = (None,20) ,Dist_val=5,detect_theshold=0)\n",
    "        sig_feats.extend(fn_feats)\n",
    "        hf_feats = kinetic_analysis(df_rg[df_rg['activity'] == 5]['pitch'],64,order=2,lf=0.1,hf=20, prom_val= (0.05,None), width_val = (None, 20) ,Dist_val=5,detect_theshold=0)\n",
    "        sig_feats.extend(hf_feats)\n",
    "\n",
    "#             print(\"tot feat len - {} \".format(len(sig_feats)))\n",
    "        sig_feats.append(file_num)\n",
    "#             print(\"tot feat len - {} \".format(len(sig_feats)))\n",
    "        sig_feats.append(pno)\n",
    "#             print(\"tot feat len - {} \".format(len(sig_feats)))\n",
    "        sig_feats.append(medication_status)\n",
    "        print(\"tot feat len - {} \".format(len(sig_feats)))\n",
    "        feat_rg.append(sig_feats)\n",
    "#         feat_rg_ts_fresh.append(tsfresh_feats_ft.values)\n",
    "        print(\"Total RG feat len - {} \".format(np.array(feat_rg).shape))\n",
    "\n",
    "            #         print(peak_valley_feats)\n",
    "#         except ValueError:\n",
    "#             print(\"Error in: {}\".format(file_num))\n",
    "#             continue\n",
    "feat_arr = np.transpose(feat_rg)\n",
    "print(len(feat_rg))\n",
    "# df_rg = pd.DataFrame(feat_rg, columns = cnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4a96c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "# Print the current working directory\n",
    "# print(\"Current working directory: {0}\".format(cwd))\n",
    "med_usage = {\n",
    "  \"0_to_1_hrs\": 0,\n",
    "  \"1_to_2_hrs\": 1,\n",
    "  \"2_to_3_hrs\": 2,\n",
    "  \"4_plus\":3,\n",
    "  \"1_to_2hrs\": 1,\n",
    "}\n",
    "channels = ['index']\n",
    "\n",
    "feat_lg = []\n",
    "\n",
    "for i in range(4):\n",
    "    pno = i+1\n",
    "    print(\"Part No: {}\".format(pno))\n",
    "    dset_csv_fpath = dset_path +'/Participant'+str(pno)+'/**/*.csv'\n",
    "    label_text_fpath = dset_path +'/Participant'+str(pno)+'/**/*.txt'\n",
    "\n",
    "    csv_files = glob.glob(dset_csv_fpath,recursive=True)\n",
    "    text_files = glob.glob(label_text_fpath,recursive=True)\n",
    "    rg_files = [s for s in csv_files if \"rg_\" in s]\n",
    "    lg_files = [s for s in csv_files if \"lg_\" in s]\n",
    "    lg_acts = [0,2,4,6,7,9]\n",
    "    rg_acts = [1,3,5,6,8,9]\n",
    "    print(\"RG cnt: {}, LG cnt: {}, Txt cnt: {}\".format(len(rg_files),len(lg_files),len(text_files)))# print(sorted(csv_files))\n",
    "    # print(glob.glob(csv_files))\n",
    "    # plot_channels = ['index','thumb','pitch','roll']\n",
    "    rg_act_no = 1\n",
    "    dist_ft = 0\n",
    "    tot_files = len(rg_files)\n",
    "    lf = 1\n",
    "    hf = 10\n",
    "    b, a = scipy.signal.butter(2, [lf*2/64, hf*2/64], 'band')\n",
    "    \n",
    "    # plot results\n",
    "    # multi_plots(data=[sig, energy, vad, detected],\n",
    "    #             titles=[\"Input signal (activity + idle)\", \"Short time energy\",\n",
    "    #                     \"Activity detection\", \"Detected signal\"],\n",
    "    #             fs=64, plot_rows=4, step=1)\n",
    "\n",
    "\n",
    "    # Extract RG FT, HH, RH task features (Mean Peak Distance, Std of distance, Mean Peak Height, Std of height, PSD band energy[2-10Hz])\n",
    "\n",
    "#     df_rg = pd.DataFrame([],columns =cnames)\n",
    "    for file_num in range(0,tot_files):\n",
    "        ch_lg = lg_files[file_num] #random.choice(rg_files)\n",
    "        df_lg = pd.read_csv(ch_lg)\n",
    "        ch_form = text_files[file_num]     #Open questionnaire and find medication intake period \n",
    "        my_file = open(ch_form, \"r\")\n",
    "        medication_status = med_usage[my_file.read().split(\"\\n\")[4].split(\"=\")[1]]\n",
    "        print(\"File No: {}, Med: {}\" .format(file_num,medication_status))\n",
    "\n",
    "    #     sig = scipy.signal.filtfilt(b, a, df_rg[df_rg['activity'] == rg_act_no][channels].values)\n",
    "        try:\n",
    "            index_sig = df_lg[df_lg['activity'] == 0]['index'].values\n",
    "            if len(index_sig)< 256:\n",
    "                print(\"File too small, skipping\")\n",
    "                continue\n",
    "            ## FT section\n",
    "            sig_feats = kinetic_analysis(index_sig,64,order=2,lf=1,hf=30, prom_val= (0.01, None), width_val = (2,20) ,Dist_val=5,detect_theshold=0)\n",
    "            \n",
    "            print(\"FT feat len - {} \".format(len(sig_feats)))\n",
    "            ch_ = [\"ax\",\"ay\",\"az\",\"gx\",\"gy\",\"gz\"]\n",
    "            # sig = butter_bandpass_filter(df_rg[df_rg['activity'] == rg_act_no]['index'].values,0.1,30,64)\n",
    "            ## Extact features for HH task in sections 1 & 2 for accel and gyro [rms accel, dominant freq, mean freq,  ]\n",
    "            hh_feats = stationary_analysis(df_lg[df_lg['activity'] == 6][ch_].values,64)\n",
    "    #             print(\"HH feat len - {} \".format(len(hh_feats)))\n",
    "            sig_feats.extend(hh_feats)\n",
    "    #             print(\"tot feat len - {} \".format(len(sig_feats)))\n",
    "\n",
    "             ## Extact features for RH task in sections 1 & 2 for accel and gyro [rms accel, dominant freq, mean freq,  ]\n",
    "            rh_feats = stationary_analysis(df_lg[df_lg['activity'] == 9][ch_].values,64)\n",
    "#             print(\"RH feat len - {} \".format(len(rh_feats)))\n",
    "            sig_feats.extend(rh_feats)\n",
    "            fn_feats = kinetic_analysis(df_lg[df_lg['activity'] == 7]['pitch'],64,order=2,lf=1,hf=20, prom_val= (0.05,None), width_val = (None,20) ,Dist_val=5,detect_theshold=0)\n",
    "            sig_feats.extend(fn_feats)\n",
    "            hf_feats = kinetic_analysis(df_lg[df_lg['activity'] == 4]['pitch'],64,order=2,lf=1,hf=20, prom_val= (0.05,None), width_val = (None,20) ,Dist_val=5,detect_theshold=0)\n",
    "            sig_feats.extend(hf_feats)\n",
    "\n",
    "\n",
    "#             print(\"tot feat len - {} \".format(len(sig_feats)))\n",
    "            sig_feats.append(file_num)\n",
    "#             print(\"tot feat len - {} \".format(len(sig_feats)))\n",
    "            sig_feats.append(pno)\n",
    "#             print(\"tot feat len - {} \".format(len(sig_feats)))\n",
    "            sig_feats.append(medication_status)\n",
    "            print(\"tot feat len - {} \".format(len(sig_feats)))\n",
    "            feat_lg.append(sig_feats)\n",
    "            print(\"Total LG feat len - {} \".format(np.array(feat_lg).shape))\n",
    "\n",
    "            #         print(peak_valley_feats)\n",
    "        except ValueError:\n",
    "            print(\"Error in: {}\".format(file_num))\n",
    "            continue\n",
    "feat_arr = np.transpose(feat_lg)\n",
    "print(len(feat_lg))\n",
    "# df_rg = pd.DataFrame(feat_rg, columns = cnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rg = pd.DataFrame(feat_rg, columns = cnames)\n",
    "df_lg = pd.DataFrame(feat_lg, columns = cnames)\n",
    "# df_rg = df_rg(df_rg['med_status']==0)\n",
    "# df_rg = df_rg.loc[df_rg['med_status'] != 0]\n",
    "# df_lg = df_lg.loc[df_lg['med_status'] != 0]\n",
    "# print(df_rg['med_status'].value_counts())\n",
    "# df_rg = df_rg.loc[df_rg['med_status'] != 2]\n",
    "# df_lg = df_lg.loc[df_lg['med_status'] != 2]\n",
    "# \n",
    "# df_rg = df_rg[df_rg['med_status']>0]\n",
    "# df_lg = df_lg[df_lg['med_status']>0]\n",
    "\n",
    "# Set medication status to binary variable 0-3hrs = 1 [on meds], 4+hrs = 0 [off meds]\n",
    "\n",
    "df_rg.loc[(df_rg['med_status'] <3,'med_status')]=1\n",
    "df_rg.loc[(df_rg['med_status'] ==3,'med_status')]=0\n",
    "df_lg.loc[(df_lg['med_status'] <3,'med_status')]=1\n",
    "df_lg.loc[(df_lg['med_status'] ==3,'med_status')]=0\n",
    "print(df_rg['med_status'].value_counts())\n",
    "print(df_lg['med_status'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "# df_rg.loc[(df_rg['med_status'] <3,'med_status')]=1\n",
    "# df_rg.loc[(df_rg['med_status'] ==3,'med_status')]=0\n",
    "# df_lg.loc[(df_lg['med_status'] <3,'med_status')]=1\n",
    "# df_lg.loc[(df_lg['med_status'] ==3,'med_status')]=0\n",
    "\n",
    "\n",
    "# Compute relative features\n",
    "\n",
    "df_rg[\"rel_seg1_acc_energy\"] = df_rg[\"rh_seg1_acc_psd\"]/df_rg[\"hh_seg1_acc_psd\"]\n",
    "df_rg[\"rel_seg1_gyr_energy\"] = df_rg[\"rh_seg1_gyr_psd\"]/df_rg[\"hh_seg1_gyr_psd\"]\n",
    "df_rg[\"rel_seg2_acc_energy\"] = df_rg[\"rh_seg2_acc_psd\"]/df_rg[\"hh_seg2_acc_psd\"]\n",
    "df_rg[\"rel_seg2_gyr_energy\"] = df_rg[\"rh_seg2_gyr_psd\"]/df_rg[\"hh_seg2_gyr_psd\"]\n",
    "df_rg[\"rel_acc_zc\"] = df_rg[\"rh_acc_zc\"]/df_rg[\"hh_acc_zc\"]\n",
    "df_rg[\"rel_gyr_zc\"] = df_rg[\"rh_gyr_zc\"]/df_rg[\"hh_gyr_zc\"]\n",
    "df_rg[\"rel_ft_energy\"] = df_rg[\"ft_dysenergy\"]/df_rg[\"ft_pdenergy\"]\n",
    "\n",
    "df_lg[\"rel_seg1_acc_energy\"] = df_lg[\"rh_seg1_acc_psd\"]/df_lg[\"hh_seg1_acc_psd\"]\n",
    "df_lg[\"rel_seg1_gyr_energy\"] = df_lg[\"rh_seg1_gyr_psd\"]/df_lg[\"hh_seg1_gyr_psd\"]\n",
    "df_lg[\"rel_seg2_acc_energy\"] = df_lg[\"rh_seg2_acc_psd\"]/df_lg[\"hh_seg2_acc_psd\"]\n",
    "df_lg[\"rel_seg2_gyr_energy\"] = df_lg[\"rh_seg2_gyr_psd\"]/df_lg[\"hh_seg2_gyr_psd\"]\n",
    "df_lg[\"rel_acc_zc\"] = df_lg[\"rh_acc_zc\"]/df_lg[\"hh_acc_zc\"]\n",
    "df_lg[\"rel_gyr_zc\"] = df_lg[\"rh_gyr_zc\"]/df_lg[\"hh_gyr_zc\"]\n",
    "df_lg[\"rel_ft_energy\"] = df_lg[\"ft_dysenergy\"]/df_lg[\"ft_pdenergy\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9223fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_lg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189387c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file if needed\n",
    "# df_rg.to_csv(dset_path+'/rg_feats.csv')\n",
    "# df_lg.to_csv((dset_path+'/lg_feats.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77601e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## SHAP Analysis (right glove) for computing feature importance\n",
    "\n",
    "feat_set_rg = df_rg\n",
    "feat_set_rg = feat_set_rg.drop(['fno', 'pno'], axis=1) \n",
    "feat_set_rg = feat_set_rg.fillna(0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "# Spliiting data into test and train sets\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(feat_set_rg.drop('med_status', axis=1), feat_set_rg['med_status'], test_size=0.20, random_state=4)\n",
    "# fitting the model\n",
    "scaler = Normalizer().fit(feat_set_rg.drop('med_status', axis=1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=700, n_jobs=-1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "# plotting feature importances\n",
    "features = feat_set_rg.drop('med_status', axis=1).columns\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "print( [features[i] for i in indices])\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test,plot_type=\"bar\",\n",
    "               show=False)\n",
    "fig = plt.gcf() # gcf means \"get current figure\"\n",
    "plt.rcParams['font.size'] = '14'\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.rcParams['axes.labelsize'] = '14'\n",
    "plt.rcParams['legend.fontsize'] = '14'\n",
    "\n",
    "\n",
    "ax = plt.gca() #gca means \"get current axes\"\n",
    "# leg = ax.legend(bbox_to_anchor=(0., 1.02, 1., .102))\n",
    "# for l in leg.get_texts(): l.set_text(l.get_text().replace('Class', 'Klasse'))\n",
    "plt.show()\n",
    "\n",
    "# perm_importance = permutation_importance(model, X_test, y_test)\n",
    "# sorted_idx = perm_importance.importances_mean.argsort()\n",
    "# plt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "# plt.xlabel(\"Permutation Importance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec75673",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SHAP Analysis (left glove) for computing feature importance\n",
    "\n",
    "feat_set_rg = df_lg\n",
    "feat_set_rg = feat_set_rg.drop(['fno', 'pno'], axis=1) \n",
    "feat_set_rg = feat_set_rg.fillna(0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "# Spliiting data into test and train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(feat_set_rg.drop('med_status', axis=1), feat_set_rg['med_status'], test_size=0.20, random_state=4)\n",
    "# fitting the model\n",
    "model = RandomForestClassifier(n_estimators=700, n_jobs=-1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "# plotting feature importances\n",
    "features = feat_set_rg.drop('med_status', axis=1).columns\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "print( [features[i] for i in indices])\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test,plot_type=\"bar\",\n",
    "               show=False)\n",
    "fig = plt.gcf() # gcf means \"get current figure\"\n",
    "plt.rcParams['font.size'] = '14'\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.rcParams['axes.labelsize'] = '14'\n",
    "plt.rcParams['legend.fontsize'] = '14'\n",
    "\n",
    "\n",
    "ax = plt.gca() #gca means \"get current axes\"\n",
    "# leg = ax.legend(bbox_to_anchor=(0., 1.02, 1., .102))\n",
    "# for l in leg.get_texts(): l.set_text(l.get_text().replace('Class', 'Klasse'))\n",
    "plt.show()\n",
    "\n",
    "# perm_importance = permutation_importance(model, X_test, y_test)\n",
    "# sorted_idx = perm_importance.importances_mean.argsort()\n",
    "# plt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "# plt.xlabel(\"Permutation Importance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a057f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.fit(X_train, y_train)\n",
    "predictions = cls.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f34233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399fab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_set_lg = df_lg\n",
    "feat_set_lg = feat_set_lg.drop(['fno', 'pno'], axis=1) \n",
    "feat_set_lg = feat_set_lg.fillna(0)\n",
    "\n",
    "# Spliiting data into test and train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(feat_set_lg.drop('med_status', axis=1), feat_set_lg['med_status'], test_size=0.20, random_state=4)\n",
    "# fitting the model\n",
    "model = RandomForestClassifier(n_estimators=700, n_jobs=-1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "# plotting feature importances\n",
    "features = feat_set_lg.drop('med_status', axis=1).columns\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "\n",
    "print( [features[i] for i in indices])\n",
    "\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test,plot_type=\"bar\",\n",
    "               show=False)\n",
    "fig = plt.gcf() # gcf means \"get current figure\"\n",
    "plt.rcParams['font.size'] = '14'\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.rcParams['axes.labelsize'] = '14'\n",
    "plt.rcParams['legend.fontsize'] = '14'\n",
    "\n",
    "\n",
    "ax = plt.gca() #gca means \"get current axes\"\n",
    "# leg = ax.legend(bbox_to_anchor=(0., 1.02, 1., .102))\n",
    "# for l in leg.get_texts(): l.set_text(l.get_text().replace('Class', 'Klasse'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = feat_set_rg.drop('med_status', axis=1).corr() # Generate correlation matrix\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x = corr.columns,\n",
    "        y = corr.index,\n",
    "        z = np.array(corr)\n",
    "    )\n",
    ")\n",
    "\n",
    "# corr2 = feat_set_lg.drop('med_status', axis=1).corr() # Generate correlation matrix\n",
    "\n",
    "# fig2 = go.Figure()\n",
    "# fig2.add_trace(\n",
    "#     go.Heatmap(\n",
    "#         x = corr2.columns,\n",
    "#         y = corr2.index,\n",
    "#         z = np.array(corr2)\n",
    "#     )\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47bf2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr2 = feat_set_lg.drop('med_status', axis=1).corr() # Generate correlation matrix\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(\n",
    "    go.Heatmap(\n",
    "        x = corr2.columns,\n",
    "        y = corr2.index,\n",
    "        z = np.array(corr2)\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.9:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "\n",
    "selected_columns_rg = feat_set_rg.drop('med_status', axis=1).columns[columns]\n",
    "df_rg_select = feat_set_rg[selected_columns_rg]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6748c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P-value code\n",
    "selected_columns_rg = selected_columns_rg[1:]\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def backwardElimination(x, Y, sl, columns):\n",
    "    numVars = len(x[0])\n",
    "    for i in range(0, numVars):\n",
    "        regressor_OLS = sm.OLS(Y, x).fit()\n",
    "        maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "        if maxVar > sl:\n",
    "            for j in range(0, numVars - i):\n",
    "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                    x = np.delete(x, j, 1)\n",
    "                    columns = np.delete(columns, j)\n",
    "                    \n",
    "    regressor_OLS.summary()\n",
    "    return x, columns\n",
    "SL = 0.05\n",
    "data_modeled, selected_columns_rg = backwardElimination(df_rg_select.iloc[:,1:].values, df_rg_select.iloc[:,0].values, SL, selected_columns_rg)\n",
    "# data_modeled, selected_columns = backwardElimination(feat_set_rg.drop('med_status', axis=1).values, feat_set_rg['med_status'], SL, selected_columns)\n",
    "result = pd.DataFrame()\n",
    "result['med_status'] = feat_set_rg['med_status']\n",
    "print(len(selected_columns_rg))\n",
    "df_rg_select = pd.DataFrame(data = data_modeled, columns = selected_columns_rg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568f1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (25, 25))\n",
    "j = 0\n",
    "for i in selected_columns_rg:\n",
    "    plt.subplot(10, 8, j+1)\n",
    "    j += 1\n",
    "    sns.distplot(feat_set_rg[i][result['med_status']==0], color='g', label = 'Pre Medication')\n",
    "    sns.distplot(feat_set_rg[i][result['med_status']==1], color='r', label = 'Post Medication')\n",
    "    plt.legend(loc='best')\n",
    "fig.suptitle('RG Data Analysis')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f6e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "for i in range(corr.shape[0]):\n",
    "    for j in range(i+1, corr.shape[0]):\n",
    "        if corr.iloc[i,j] >= 0.9:\n",
    "            if columns[j]:\n",
    "                columns[j] = False\n",
    "selected_columns_lg = feat_set_lg.drop('med_status', axis=1).columns[columns]\n",
    "df_lg_select = feat_set_lg[selected_columns_lg]\n",
    "selected_columns_lg = selected_columns_lg[1:]\n",
    "# P-value code\n",
    "SL = 0.05\n",
    "data_modeled, selected_columns_lg = backwardElimination(df_lg_select.iloc[:,1:].values, df_lg_select.iloc[:,0].values, SL, selected_columns_lg)\n",
    "# data_modeled, selected_columns = backwardElimination(feat_set_rg.drop('med_status', axis=1).values, feat_set_rg['med_status'], SL, selected_columns)\n",
    "result = pd.DataFrame()\n",
    "result['med_status'] = feat_set_lg['med_status']\n",
    "df_lg_select = pd.DataFrame(data = data_modeled, columns = selected_columns_lg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (25, 25))\n",
    "j = 0\n",
    "for i in data_lg.columns:\n",
    "    plt.subplot(10, 8, j+1)\n",
    "    j += 1\n",
    "    sns.distplot(data_lg[i][result['med_status']==0], color='g', label = 'Pre Medication')\n",
    "    sns.distplot(data_lg[i][result['med_status']==1], color='r', label = 'Post Medication')\n",
    "    plt.legend(loc='best')\n",
    "fig.suptitle('LG Data Analysis')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9fd067",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_set_rg_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1962f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_rg[df_rg['activity'] == 6][ch_]\n",
    "fig = px.box(data_frame = df_lg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'hf_mean_p2p_dist'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "#     legend=dict(\n",
    "#     yanchor=\"bottom\",\n",
    "#     y=0.0,\n",
    "#     xanchor=\"right\",\n",
    "# #     x=0.01\n",
    "#     ),\n",
    "    title={\n",
    "        'text': \"Left Glove - Hand Flip Mean Peak to Peak distance\",\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'bottom'},\n",
    "    xaxis_title=\"Participant\",\n",
    "    yaxis_title=\"Sample difference\",\n",
    "    font=dict(\n",
    "        family=\"Times New Roman, monospace\",\n",
    "        size=18,\n",
    "        color=\"Black\"\n",
    "    ))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffdc4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.violin(df_lg, y=\"hf_mean_p2p_dist\", x=\"pno\", color=\"med_status\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded6341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# tips = sns.load_dataset('tips')\n",
    "\n",
    "# Define some hatches\n",
    "hatches = cycle(['///', 'x'])\n",
    "\n",
    "# Boxplot\n",
    "ax = sns.boxplot(x=\"pno\", y=\"hf_mean_p2p_dist\", hue=\"med_status\", data=df_lg)\n",
    "for i, patch in enumerate(ax.artists):\n",
    "    # Boxes from left to right\n",
    "    hatch = next(hatches)\n",
    "    patch.set_hatch(hatch)\n",
    "\n",
    "ax.axes.set_title(\"Left Hand Flip peak to peak distance\", fontsize=16)\n",
    "ax.set_xlabel(\"Participants\", fontsize=14)\n",
    "ax.set_ylabel(\"Mean peak to peak distance\", fontsize=14)\n",
    "ax.legend([],[], frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e6acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.violin(df_lg, y=\"rh_seg2_gyr_mf\", x=\"pno\", color=\"med_status\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610a2119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# tips = sns.load_dataset('tips')\n",
    "\n",
    "# Define some hatches\n",
    "hatches = cycle(['///', 'x'])\n",
    "\n",
    "# Boxplot\n",
    "ax = sns.boxplot(x=\"pno\", y=\"rh_seg2_gyr_mf\", hue=\"med_status\", data=df_rg)\n",
    "for i, patch in enumerate(ax.artists):\n",
    "    # Boxes from left to right\n",
    "    hatch = next(hatches)\n",
    "    patch.set_hatch(hatch)\n",
    "\n",
    "ax.axes.set_title(\"Right Hand resting hand mean frequency\", fontsize=16)\n",
    "ax.set_xlabel(\"Participants\", fontsize=14)\n",
    "ax.set_ylabel(\"Mean Frequency (Hz)\", fontsize=14)\n",
    "# ax.legend([],[], frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig.update_layout(\n",
    "#     legend=dict(\n",
    "#     yanchor=\"bottom\",\n",
    "#     y=0.0,\n",
    "#     xanchor=\"right\",\n",
    "# #     x=0.01\n",
    "#     ),\n",
    "    title={\n",
    "        'text': \"Left Glove - Hand Flip Mean Peak to Peak distance\",\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'bottom'},\n",
    "    xaxis_title=\"Participant\",\n",
    "    yaxis_title=\"Relative Energy (a.u)\",\n",
    "    font=dict(\n",
    "        family=\"Times New Roman, monospace\",\n",
    "        size=18,\n",
    "        color=\"Black\"\n",
    "    ))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a597db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'rel_acc_zc'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68720662",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'rh_seg2_acc_mf'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_lg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'hf_std_p2p_dist'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1597201",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'rh_seg2_gyr_psd'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a5700",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'hh_seg2_gyr_psd'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd1e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'rh_seg2_gyr_df'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aefcccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'rh_seg2_gyr_df'\n",
    "       ,color= 'med_status'\n",
    "#        ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7493a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'hh_seg2_gyr_df'\n",
    "       ,color= 'med_status'\n",
    "#        ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf3f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'hh_seg2_acc_df'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22940f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'rh_seg2_gyr_df'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e6ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_lg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'hh_seg2_gyr_df'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ba7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_lg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'hf_pdenergy'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa4f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_lg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'hf_pdenergy'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc009e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'rel_seg2_gyr_energy'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_lg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'rel_seg2_gyr_energy'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'rel_seg2_gyr_energy'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead71488",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'rh_seg2_acc_psd'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb936df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_lg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'rh_seg2_acc_psd'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277548a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'rh_seg2_gyr_psd'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'rel_seg1_gyr_energy'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'hh_seg2_acc_dysenergy'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eaadac",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'hh_gyr_zc'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76fbd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data_frame = df_rg\n",
    "       ,x = 'pno'\n",
    "       ,y = 'hh_seg1_mean_freq'\n",
    "       ,color= 'med_status'\n",
    "       ,points=\"all\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12b91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_rg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55336777",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b8388",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_rg = pd.DataFrame(feat_rg,columns = ['ax','ay','az','gx','gy','gz','index','thumb','middle'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644916dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_rms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4fda0b754a13b9dfec1029e35c98a5438678fa8b1ab75998cbf927350000ebfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
